defaults:
  - _self_
  - tokenizer: default
  - world_model: default
  - actor_critic: default
  - env: default
  - datasets: default

env_actions: '{"num_actions": 256, "num_continuous": 3}'  # module train_cloud argparse workaround

wandb:
  mode: online
  project: iris
  entity: null
  name: "orig_lambda"
  group: null
  tags: null
  notes: ""

initialization:
  storage_prefix: "" # run prefix on bucket to load agent weights. Prioritized over path
  path_to_checkpoint: null #C:\Users\Mykhailo_Tkachuk\PycharmProjects\Brawl-Stars-AI\outputs\2023-08-27\09-17-56\checkpoints\last.pt
  load_tokenizer: False
  load_world_model: False
  load_actor_critic: True

common:
  epochs: 600
  device: cpu
  do_checkpoint: True
  seed: 0
  sequence_length: ${world_model.max_blocks}
  resume: False # run prefix on bucket or False. Loads full checkpoint

collection:
  train:
    num_envs: 1
    stop_after_epochs: 500
    num_episodes_to_save: 10
    config:
      epsilon: 0.01  # random move proba
      should_sample: True
      temperature: 1.0
      num_steps: 400
      burn_in: ${training.actor_critic.burn_in}
      speed_constraint: 2 # it per sec
  test:
    num_envs: 1
    num_episodes_to_save: ${collection.train.num_episodes_to_save}
    config:
      epsilon: 0.0
      should_sample: True
      temperature: 0.5
      num_episodes: 16
      burn_in: ${training.actor_critic.burn_in}
      speed_constraint: ${collection.train.config.speed_constraint}

cloud:
  region_name: "us-east-1"
  bucket_name: "brawl-stars-iris"
  instance_id: "i-0e9cb879b7a6f87c7"
  key_file: "C:\\Users\\Mykhailo_Tkachuk\\keypair1.pem"
  log_metrics: "logs/metrics.json"  # used for logging
  log_reconstruction: "logs/reconstruction.jpg"  # used for logging

training:
  should: True
  on_cloud: True
  epochs_per_job: 1 # number of epochs to be run per job submission
  learning_rate: 0.00003
  sampling_weights: [0.125, 0.125, 0.25, 0.5]
  tokenizer:
    batch_num_samples: 8 # max 24 # old 256
    grad_acc_steps: 1
    max_grad_norm: 10.0
    start_after_epochs: 99999
    steps_per_epoch: 200
  world_model:
    batch_num_samples: 8 # max 14 # old 64
    grad_acc_steps: 1
    max_grad_norm: 10.0
    weight_decay: 0.01
    start_after_epochs: 99999
    steps_per_epoch: 200
  actor_critic:
    batch_num_samples: 64 # max 24 # old 64
    grad_acc_steps: 1
    max_grad_norm: 10.0
    start_after_epochs: 0
    steps_per_epoch: 100
    imagine_horizon: ${common.sequence_length}
    burn_in: 60
    gamma: 0.995
    lambda_: 0.95
    entropy_weight: 0.001

evaluation:
  should: False
  every: 5
  tokenizer:
    batch_num_samples: ${training.tokenizer.batch_num_samples}
    start_after_epochs: ${training.tokenizer.start_after_epochs}
    save_reconstructions: True
  world_model:
    batch_num_samples: ${training.world_model.batch_num_samples}
    start_after_epochs: ${training.world_model.start_after_epochs}
  actor_critic:
    num_episodes_to_save: ${training.actor_critic.batch_num_samples}
    horizon: ${training.actor_critic.imagine_horizon}
    start_after_epochs: ${training.actor_critic.start_after_epochs}
